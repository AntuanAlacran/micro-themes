{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Backprop Vectorizado.ipynb","provenance":[{"file_id":"1VlcNOiRBzGZZjPTVzVZpXUyDTMOfIJEJ","timestamp":1651522137769}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMwH/20ISfqBh92KvFPBYFI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Una sola observación"],"metadata":{"id":"YjSDwhzxRS4k"}},{"cell_type":"markdown","source":["Dada una función de error, que podemos definir como el error cuadrático medio:<br>\n","$\\begin{align}\n","J(\\Theta)=\\frac{1}{m}\\sum_{i=1}^m(ŷ^i-y^i)^2\n","\\end{align}$\n","<br>donde $y^i$ y $ŷ^i$ representan el valor de salida real y la salida estimada por la red, respectivamente.\n","\n","La idea de backpropagation es calcular los pesos ($\\Theta^l_{ij}$) que minimicen la función de error, i.e.:<br>\n","$\\begin{align}\n","\\min_\\Theta J(\\Theta)\n","\\end{align}$\n","<br>\n","y en este caso utilizaremos descenso de gradiente, por lo que se deben calcular las derivadas parciales de la función de error, tal que:<br>\n","<font color='BlueViolet'>\n","$\\begin{align}\n","\\frac{\\partial}{\\partial\\Theta^l_{ij}}J(\\Theta)\n","\\end{align}$\n","</font><br>"],"metadata":{"id":"kIv7rHHRRS0-"}},{"cell_type":"markdown","source":["Pensemos un problema con un conjunto de datos de una sola observación $(x^1,y^1)$, con la siguiente estructura de red:\n"],"metadata":{"id":"rBodryyAQEbx"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://i.postimg.cc/QCSkJNhQ/Net01.png\" alt=\"Red Neuronal\" width=\"50%\">\n","</center>"],"metadata":{"id":"8SLF5VaemxRP"}},{"cell_type":"markdown","source":["Al igual que en el ejemplo anterior de Backpropagation, lo primero es llevar a cabo el Forward propagation para calcular las activaciones de cada capa.\n","<center>\n","<img src=\"https://i.postimg.cc/pr0zn0Jk/Net02.png\" alt=\"Red Neuronal\" width=\"30%\">\n","</center>"],"metadata":{"id":"GurNEq6lRSxh"}},{"cell_type":"markdown","source":["## Forward propagation\n","\n","$\\begin{align}\n","a^1 &= x  \\\\\n","z^2 &= \\Theta^1a^1 \\\\\n","a^2 &= g(z^2) \\:\\: \\text{agregar} \\:\\: a^2_0 \\\\\n","z^3 &= \\Theta^2a^2 \\\\\n","a^3 &= g(z^3) \\:\\: \\text{agregar} \\:\\: a^3_0 \\\\\n","z^4 &= \\Theta^3a^3 \\\\\n","a^4 &= h_\\Theta(x) = g(z^4) \\\\\n","\\end{align}$\n","<br>\n"],"metadata":{"id":"yrBOiqjSoM0X"}},{"cell_type":"markdown","source":["## Backpropagation\n","Para el cálculo de los gradientes, tomemos en cuenta a $\\delta^l_j$ como el error asociado a la neurona $j$ de la capa $l$, es decir el error asociado al nodo $a^l_j$"],"metadata":{"id":"5zLm8oGioU_m"}},{"cell_type":"markdown","source":["1) Para cada neurona de salida (última capa):<br>\n","\n","$\\delta^4_j = a^4_j - y_j$\n","\n","Pero puede expresarse vectorialmente como:<font color='Crimson'><br>\n","$\\delta^4 = a^4 - y$\n","</font>\n","\n","2) Para las capas previas:<br>\n","$\\begin{align}\n","\\delta^3 &= (\\Theta^3)^T \\delta^4 .* g'(z^3)  \\\\\n","\\delta^2 &= (\\Theta^2)^T \\delta^3 .* g'(z^2)  \\\\\n","\\end{align}$ <br><br>\n","donde $g'(z^3)$ y $g'(z^3)$ representa la derivada de la función de activación en la 2a y 3a capa, que para el caso de la función logística es:<br>\n","$\\begin{align}\n","g'(z^3) &= a^3.*(1-a^3)  \\\\\n","g'(z^2) &= a^2.*(1-a^2)  \\\\\n","\\end{align}$ <br><br>\n","y por lo tanto:<font color='SlateBlue'><br>\n","$\\begin{align}\n","\\delta^3 &= (\\Theta^3)^T \\delta^4 .* \\left(a^3 .* (1-a^3)\\right)  \\\\\n","\\delta^2 &= (\\Theta^2)^T \\delta^3 .* \\left(a^2 .* (1-a^2)\\right)  \\\\\n","\\end{align}$ <br><br>\n","</font>\n","\n"],"metadata":{"id":"5nM9VD3VrU9Y"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"1WhiAeak4mUB"}},{"cell_type":"markdown","source":["# Con m observaciones\n","Partiendo de un conjunto de entrenamiento tal que:<br>\n","$\\begin{align}\n","\\text{Training set}=\\{\\left(x^1, y^1\\right), \\left(x^2, y^2\\right),...,\\left(x^m, y^m\\right)\\}\n","\\end{align}$\n"],"metadata":{"id":"u_q1V1PTFunE"}},{"cell_type":"markdown","source":["Inicializar <font color='Salmon'>$\\Delta^l_{i,j}=0$</font> $, \\: \\forall\\: l,i,j$, que será usado para calcular el gradiente\n","$\\begin{align}\n","\\frac{\\partial}{\\partial\\Theta^l_{ij}}J(\\Theta)\n","\\end{align}$\n","<br>"],"metadata":{"id":"M_OhWfiD5mu0"}},{"cell_type":"markdown","source":["Por cada observación del conjunto de entrenamiento, es decir:<br>\n","$\\text{Para } \\: i=1 \\:\\: \\text{hasta} \\:\\: m:$"],"metadata":{"id":"C6dSDawo8Vo_"}},{"cell_type":"markdown","source":["## Forward propagation\n","\n","$\\begin{align}\n","a^1 &= x^i  \\\\\n","z^2 &= \\Theta^1a^1 \\\\\n","a^2 &= g(z^2) \\:\\: \\text{agregar} \\:\\: a^2_0 \\\\\n","z^3 &= \\Theta^2a^2 \\\\\n","a^3 &= g(z^3) \\:\\: \\text{agregar} \\:\\: a^3_0 \\\\\n","z^4 &= \\Theta^3a^3 \\\\\n","a^4 &= h_\\Theta(x^i) = g(z^4) \\\\\n","\\end{align}$\n","<br>"],"metadata":{"id":"vTW3ctSm87c_"}},{"cell_type":"markdown","source":["## Backpropagation\n"],"metadata":{"id":"-XZTRONC9Zb3"}},{"cell_type":"markdown","source":["1) Para cada neurona de salida (última capa):<br>\n","$\\delta^4_j = a^4_j - y_j$  \\\\\n","\n","2) Para las capas previas:<br>\n","$\\begin{align}\n","\\delta^3 &= (\\Theta^3)^T \\delta^4 .* \\left(a^3 .* (1-a^3)\\right)  \\\\\n","\\Delta^l_{i,j} :&= \\Delta^l_{i,j} + a^l_j \\delta^{(l+1)}_i  \\\\\n","\\Delta^3_{i,j} :&= \\Delta^3_{i,j} + a^3_j \\delta^4_i \n","\\end{align}$\n","\n","<br>\n","\n","$\\begin{align}\n","\\delta^2 &= (\\Theta^2)^T \\delta^3 .* \\left(a^2 .* (1-a^2)\\right)  \\\\\n","\\Delta^l_{i,j} :&= \\Delta^l_{i,j} + a^l_j \\delta^{(l+1)}_i  \\\\\n","\\Delta^2_{i,j} :&= \\Delta^2_{i,j} + a^2_j \\delta^3_i \n","\\end{align}$\n"],"metadata":{"id":"P7MOrx2L6eOS"}},{"cell_type":"markdown","source":["Lo mismo, podemos expresarlo de forma vectorizada, tal que:<br>\n","1) Para cada neurona de salida (última capa):<br>\n","<font color='Crimson'>\n","$\\begin{align}\n","\\delta^4 &= a^4 - y^i   \\\\\n","\\Delta^4 &= \\delta^4\n","\\end{align}$\n","</font>\n","<br><br>\n","\n","2) Para las capas previas:<br>\n","<font color='SlateBlue'><br>\n","$\\begin{align}\n","\\delta^3 &= (\\Theta^3)^T \\delta^4 .* \\left(a^3 .* (1-a^3)\\right)\n","\\end{align}$\n","<font color='LimeGreen'><br>\n","$\\begin{align}\n","\\Delta^l &:= \\Delta^l + \\delta^{(l+1)}(a^l)^T   \\\\\n","\\Delta^3 &:= \\Delta^3 + \\delta^4(a^3)^T   \\\\\n","\\end{align}$ <br>\n","\n","<font color='SlateBlue'><br>\n","$\\begin{align}\n","\\delta^2 &= (\\Theta^2)^T \\delta^3 .* \\left(a^2 .* (1-a^2)\\right)  \\\\\n","\\end{align}$\n","<font color='LimeGreen'><br>\n","$\\begin{align}\n","\\Delta^l &:= \\Delta^l + \\delta^{(l+1)}(a^l)^T   \\\\\n","\\Delta^2 &:= \\Delta^2 + \\delta^3(a^2)^T   \\\\\n","\\end{align}$ <br>\n","</font>"],"metadata":{"id":"C6GtLCRxCDQa"}},{"cell_type":"markdown","source":["Finalmente, se promedia el resultado del Delta en función del número de observaciones. Con ello, se obtiene el gradiente como:<br>\n","<font color='BlueViolet'>\n","$\\begin{align}\n","\\frac{\\partial}{\\partial\\Theta^l_{ij}}J(\\Theta) = \\frac{1}{m}\\Delta^l_{i,j}\n","\\end{align}$\n","</font><br>"],"metadata":{"id":"cm0_MG1sFX7t"}},{"cell_type":"markdown","source":["```\n","NG, Andrew. Machine Learning. Stanford University, Coursera (2015).\n","https://www.coursera.org/learn/machine-learning\n","\n","```"],"metadata":{"id":"z-X6vtPMVC1O"}}]}